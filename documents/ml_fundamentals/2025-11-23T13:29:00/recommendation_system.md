# **ML Fundamental: 추천 시스템 및 딥러닝 아키텍처**

광고 플랫폼 기업의 핵심은 사용자에게 가장 관련성 높은 광고를 추천하고 순위를 매기는 것입니다. 이 섹션에서는 이를 가능하게 하는 핵심적인 딥러닝 및 모델링 기법을 다룹니다.

## **1\. 임베딩 (Embeddings): 고차원 데이터의 압축된 표현**

### **A. 임베딩의 정의 및 필요성**

**임베딩**은 사용자 ID, 아이템 ID, 광고 카테고리 등 \*\*높은 카디널리티(High Cardinality)\*\*를 가진 범주형 변수를 낮은 차원의 **연속형 벡터 공간**으로 매핑하는 과정입니다.

* **문제 제기:** 사용자 ID가 수억 개일 경우, One-Hot Encoding을 사용하면 모델 입력 차원이 너무 커져 계산 비효율성과 **희소성(Sparsity)** 문제가 발생합니다. 희소성은 모델이 패턴을 학습하기 어렵게 만듭니다.
* **해결책:** 임베딩은 이 문제를 해결하여 효율적인 학습을 가능하게 합니다.
* **벡터 공간의 의미:** 임베딩 벡터 공간에서는 **의미적 유사성**이 벡터 간의 거리(Distance)로 표현됩니다.
  * *예시:* 아이돌 그룹 A의 노래를 들은 사용자와 그룹 B의 노래를 들은 사용자의 벡터는 가까이 위치합니다. 이는 모델이 ID 자체보다 ID 간의 \*\*관계와 패턴(선호도)\*\*을 학습하도록 돕습니다.

### **B. 임베딩 학습의 원리 (행렬 분해와의 연결)**

임베딩 벡터는 일반적으로 다음과 같은 두 가지 방법으로 학습됩니다:

1. **사전 학습 (Pre-training) \- 행렬 분해:** 사용자-아이템 상호작용 행렬 $R$을 두 개의 저랭크 행렬(사용자 행렬 $P$, 아이템 행렬 $Q$)의 곱으로 분해하는 **협업 필터링(Collaborative Filtering)** 방식을 통해 초기 임베딩 벡터를 얻습니다. 여기서 $P$의 행이 사용자 임베딩, $Q$의 열이 아이템 임베딩이 됩니다.
2. **모델 학습 중 학습 (Learning during Training):** 딥러닝 모델에 임베딩 레이어(Embedding Layer)를 추가하고, 백프로파게이션(Backpropagation) 과정에서 예측 오차를 줄이는 방향으로 임베딩 벡터 자체를 가중치처럼 업데이트합니다. 대부분의 최신 추천 모델에서 이 방식을 사용합니다.

### **C. 임베딩의 활용**

1. **유사성 계산:** 두 임베딩 벡터 간의 내적(Dot Product)이나 코사인 유사도(Cosine Similarity)를 계산하여 사용자-아이템, 아이템-아이템 간의 선호도 또는 유사도를 예측합니다.
2. **피처 상호작용 (Feature Interaction):** 임베딩 벡터들을 결합(Concatenation 또는 Element-wise Product)하여 딥러닝 레이어에 입력함으로써, 고차원적인 피처 간의 상호작용 패턴을 학습하게 합니다.

## **2\. Factorization Machines (FM) 및 FFM: 희소성 극복과 상호작용 모델링**

FM은 전통적인 선형 모델이 희소 데이터에서 상호작용을 포착하지 못하는 한계를 극복하기 위해 설계되었습니다.

### **A. FM (Factorization Machines)의 핵심 흐름**

선형 모델은 피처 $x_i$와 $x_j$가 동시에 등장했을 때의 상호작용 가중치 $w_{ij}$를 학습합니다. 하지만 희소 데이터에서는 $x_i x_j$ 조합이 학습 데이터에 없으면 $w_{ij}$를 학습할 수 없습니다 (Cold Start 문제).

FM은 이 문제를 \*\*잠재 벡터(Latent Vector)\*\*를 도입하여 해결합니다.

1. **잠재 벡터 학습:** 각 피처 $i$에 대해 $k$차원의 잠재 벡터 $\mathbf{v}_i$를 학습합니다.
2. **상호작용 추정:** 두 피처 $i$와 $j$의 상호작용 가중치($$w_{ij}$$
   )를 이들의 잠재 벡터 내적($$\langle \mathbf{v}_i, \mathbf{v}_j \rangle$$
   )으로 대체합니다.

$$\hat{y}(\mathbf{x}) = \underbrace{w_0 + \sum_{i=1}^{n} w_i x_i}_{\text{선형 컴포넌트}} + \underbrace{\sum_{i=1}^{n} \sum_{j=i+1}^{n} \langle \mathbf{v}_i, \mathbf{v}_j \rangle x_i x_j}_{\text{상호작용 컴포넌트 (2차)}}$$

* **장점 (정보 전이 효과):** 피처 $i$와 $j$가 한 번도 함께 나타나지 않았더라도, 피처 $i$가 다른 피처 $l$과 상호작용한 기록(즉, $\mathbf{v}_i$ 학습에 기여한 기록)이 있다면, 이 $\mathbf{v}_i$를 통해 $i$와 $j$의 상호작용($$\langle \mathbf{v}_i, \mathbf{v}_j \rangle$$
  )을 \*\*추정(Transfer)\*\*할 수 있습니다. 이는 희소성 문제에 매우 강력한 해결책을 제시합니다.

### **B. FFM (Field-Aware Factorization Machines)의 정교화**

FFM은 FM을 확장하여 피처가 속한 **필드(Field)** 정보를 추가로 고려합니다.

* **개념:** FFM에서는 각 피처 $i$가 다른 필드 $j$에 대해 서로 다른 잠재 벡터 $\mathbf{v}_{i, f_j}$를 가집니다.
* *예시:* 피처 '사용자 성별'이 '광고 카테고리' 필드와 상호작용할 때의 잠재 벡터($\mathbf{v}_{\text{성별}, \text{카테고리}}$)와 '사용자 연령' 필드와 상호작용할 때의 잠재 벡터($\mathbf{v}_{\text{성별}, \text{연령}}$)가 다릅니다.
* **결과:** 이는 상호작용의 \*\*맥락(Context)\*\*을 모델링하여 FM보다 더 정교한 예측을 가능하게 합니다. 다만, 계산 복잡도는 FM보다 훨씬 높습니다.

## **3\. Wide & Deep Learning: 암기와 일반화의 결합**

Google이 제안한 하이브리드 추천 시스템 아키텍처로, 모델의 **암기(Memorization)** 능력과 **일반화(Generalization)** 능력을 동시에 활용합니다.

### **A. Wide Component (암기)**

* **목적:** 데이터에서 **자주 발생하는 피처 또는 피처 조합**을 직접적으로 학습합니다. 이는 정확하고 해석 가능한 암기 능력을 제공합니다.
* **입력:** 주로 **원시 입력(Raw Input)** 및 \*\*수동 생성된 교차 피처(Cross-Product Features)\*\*를 사용합니다.
  * *교차 피처 예시:* AND('국민은행 사용자', '여행 광고 클릭')처럼 특정 조합이 발생했을 때 1의 값을 가지는 피처를 수동으로 생성하여 모델에 입력합니다.

### **B. Deep Component (일반화)**

* **목적:** 임베딩 벡터를 입력으로 받아 다층 신경망(DNN)을 통해 **데이터에 없는 새로운 피처 조합**이나 복잡한 비선형 관계를 학습합니다. 이는 일반화 능력을 높여 새로운 사용자나 아이템에도 잘 작동하도록 합니다.
* **입력:** 모든 **범주형 피처의 임베딩 벡터**와 **연속형 피처**를 연결(Concatenate)하여 사용합니다.

### **C. 결합 및 핵심 이점**

* **결합:** 두 컴포넌트의 출력(Wide의 선형 예측 값 \+ Deep의 DNN 출력 값)을 최종적으로 **로지스틱 회귀**를 통해 결합하여 예측을 수행합니다.
* **핵심 이점:**
  1. **암기 (**$W$**):** 높은 CTR을 보이는 '특정 사용자 ID \+ 특정 광고 캠페인 ID'와 같은 **명확한 규칙**을 놓치지 않습니다.
  2. **일반화 (**$D$**):** '30대 남성 사용자'와 'IT 기기 카테고리 광고'와 같은 **일반적인 잠재 패턴**을 학습하여, 데이터셋에 없는 새로운 조합에 대해서도 합리적인 예측을 할 수 있습니다.

## **4\. Attention Mechanism (기본 개념): 시퀀스 데이터에서의 선택적 집중**

최근 딥러닝 기반 추천 시스템에서 필수적인 요소로, 사용자 행동 시퀀스 데이터에서 **어떤 정보가 현재의 예측에 가장 중요한지** 가중치를 부여하는 메커니즘입니다.

### **A. Query, Key, Value (QKV) 관점에서의 이해**

Attention은 '현재의 요구(Query)'를 기준으로 '과거의 정보(Key)'를 검색하여 '가중치가 적용된 정보(Value)'를 얻는 과정으로 이해할 수 있습니다.

* **Query (질의):** **현재 예측하고자 하는 대상** (예: 현재 노출된 광고의 임베딩 벡터).
* **Key (열쇠):** **과거의 모든 시퀀스 정보** (예: 사용자의 과거 100개 클릭 기록 각각의 임베딩 벡터).
* **Value (값):** Key와 동일하며, 가중치를 적용할 정보의 내용.

### **B. 핵심 역할**

1. **가중치 계산:** Query와 Key 간의 유사도를 측정하여 \*\*어텐션 가중치(Attention Weight)\*\*를 계산합니다. 유사도가 높을수록 더 높은 가중치가 부여됩니다.
2. **선택적 집중:** 이 가중치를 Value에 적용하여 시퀀스 정보들을 가중합합니다. 결과적으로 현재 Query와 **가장 관련성이 높은 과거 행동 기록**에 모델이 집중하게 됩니다.
3. **동적 가중치:** 가중치가 입력 데이터(Query)에 따라 **동적으로 변하기 때문에**, 기존의 고정 가중치 방식보다 훨씬 유연하고 정교한 맥락 파악이 가능합니다.

## **5\. 예상 객관식/단답형 질문 (Self-Test)**

1. **질문:** Factorization Machines(FM)이 희소 데이터 환경에서 **선형 회귀 모델**보다 피처 상호작용을 더 잘 모델링하는 이유를 \*\*'잠재 벡터'\*\*와 **'정보 전이'** 관점에서 설명하세요.
   * **답변 예시:** 선형 회귀는 상호작용 조합이 데이터에 없으면 학습이 불가능합니다. 반면 FM은 각 피처에 대한 잠재 벡터($\mathbf{v}_i$)를 학습하며, 이 잠재 벡터는 다른 피처들과의 상호작용 데이터로부터 **정보를 공유**받아 학습됩니다. 따라서 희소한 조합에 대해서도 이 공유된 벡터를 통해 상호작용 가중치($$\langle \mathbf{v}_i, \mathbf{v}_j \rangle$$
     )를 **전이(Transfer)** 받아 추정할 수 있습니다.
2. **질문:** Wide & Deep Learning 아키텍처에서 **Wide Component**가 담당하는 역할과 **Deep Component**가 담당하는 역할의 차이점을 설명하고, 왜 이 둘을 결합해야 하는지 설명하세요.
   * **답변 예시:** Wide Component는 **암기(Memorization)** 역할을 담당하여 빈번하게 발생하는 피처 조합을 정확하게 포착합니다. Deep Component는 **일반화(Generalization)** 역할을 담당하여 임베딩을 통해 복잡하고 새로운 피처 관계를 비선형적으로 학습합니다. 둘을 결합하는 이유는, 암기 능력을 통해 정확도를 유지하고, 일반화 능력을 통해 새로운 데이터에 대한 유연성을 확보하여 상호 보완적인 성능을 얻기 위함입니다.
3. **질문:** **Attention Mechanism**에서 'Query' 역할을 하는 정보와 'Key' 역할을 하는 정보를 **사용자 행동 시퀀스 추천 시스템** 관점에서 각각 예시 들어 설명하세요.
   * **답변 예시:**
     * **Query:** **현재 예측을 수행해야 하는 대상** (예: 현재 사용자에게 노출된 광고 아이템의 임베딩 벡터).
     * **Key:** **참고가 필요한 과거 정보 시퀀스** (예: 사용자가 과거에 클릭했던 모든 아이템/광고 기록 각각의 임베딩 벡터).
4. **질문:** FFM(Field-Aware Factorization Machines)에서 Field 정보를 고려하는 것이 FM에 비해 어떤 이점을 가져오나요?
   * **답변 예시:** FM은 모든 피처에 대해 하나의 잠재 벡터를 사용하지만, FFM은 각 피처가 **다른 필드의 피처와 상호작용할 때마다 다른 잠재 벡터**를 가집니다. 이는 상호작용의 \*\*맥락(Context)\*\*을 더 정교하게 모델링할 수 있게 하여 예측 정확도를 높입니다.
