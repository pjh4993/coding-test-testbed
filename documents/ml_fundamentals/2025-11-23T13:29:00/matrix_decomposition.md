# **ML Fundamental: 행렬 분해와 저랭크 근사 (SVD, PCA, LoRA)**

이 섹션은 행렬의 **랭크(Rank)** 개념을 바탕으로, 전통적인 차원 축소 기법인 **SVD/PCA**부터 최신 모델 최적화 기법인 **LoRA**까지 연결하는 자료입니다. 이는 **광고 플랫폼 회사**와 같은 Ad-Tech/MLOps 환경에서 모델 경량화 및 효율화를 이해하는 데 핵심적인 지식입니다.

## **1\. 행렬의 랭크 (Matrix Rank)와 정보 압축**

### **A. 행렬 랭크의 정의**

행렬의 \*\*랭크(Rank)\*\*는 행렬의 열(Column) 벡터 또는 행(Row) 벡터 중 \*\*선형 독립(Linearly Independent)\*\*인 벡터의 최대 개수입니다. 이는 행렬 $W$에 대한 선형 변환을 통해 만들 수 있는 벡터 공간의 \*\*차원(Dimensionality)\*\*을 의미합니다. 랭크 $r$은 행렬의 최소 차원(

$$r \le \min(\text{행 개수}, \text{열 개수})$$
)을 넘을 수 없으며, 최대치에 도달했을 때 **Full Rank**라고 부릅니다.

* **해석: 랭크와 정보의 중복성**
  * 랭크가 낮다는 것은 행렬을 구성하는 벡터들 사이에 **선형 종속(Linear Dependence)** 관계가 많다는 것을 의미합니다. 즉, 한 벡터를 다른 벡터들의 선형 결합으로 표현할 수 있습니다.
  * 이처럼 행렬의 많은 부분이 중복된 정보(redundant information)를 포함하고 있으므로, 이 중복된 부분을 제거하고 \*\*실질적으로 필요한 독립적인 축(랭크 $k$)\*\*만 남겨도 행렬의 핵심적인 구조와 정보를 보존할 수 있습니다.

### **B. 저랭크 근사 (Low-Rank Approximation)**

높은 랭크를 가진 원래 행렬 $W$를, 더 작은 랭크 $k$를 가진 두 행렬 $U$와 $V^T$의 곱으로 근사하는 기법(

$$W \approx U V^T$$
)입니다. 여기서 $k$는 $W$의 원래 랭크보다 훨씬 작은 값입니다.

* **목적:** 데이터/모델의 **차원을 축소**하거나 **경량화**하여 계산 효율성을 높이고 노이즈를 제거하는 데 사용됩니다.
* **실용적인 이점:**
  1. **메모리 및 저장 공간 절약:** $d \times k$ 행렬 대신 $d \times r$과 $r \times k$ 두 개의 작은 행렬만 저장하여, 특히 거대한 모델 가중치 저장에 필요한 공간을 크게 줄일 수 있습니다.
  2. **계산 효율성 증대:** 행렬 곱셈 시 계산 복잡도가 $O(dk \min(d,k))$에서 $O(dkr)$로 줄어들어 연산 속도가 빨라집니다. 이는 실시간 추론(Inference) 환경에서 지연 시간을 줄이는 데 결정적입니다.
  3. **강건성(Robustness) 향상:** 가장 작은 특잇값에 해당하는 성분들은 종종 데이터의 노이즈를 나타냅니다. 저랭크 근사를 통해 이 성분들을 제거함으로써 모델이 불필요한 노이즈에 과적합되는 것을 방지하고 일반화 성능을 높일 수 있습니다.

## **2\. SVD (Singular Value Decomposition)와 PCA (Principal Component Analysis)**

SVD는 행렬 분해의 수학적 기반을 제공하며, PCA는 SVD의 원리를 활용하여 데이터의 주요 특징을 추출하는 차원 축소 기법입니다.

### **A. SVD (특잇값 분해)**

임의의 행렬 $M$을 세 개의 행렬의 곱으로 분해합니다.

$$M = U \Sigma V^T$$

* $U$: 왼쪽 특이 벡터 (Orthogonal Matrix), 행 공간의 기저
* $\Sigma$: \*\*특잇값(Singular Value)\*\*이 대각선에 내림차순으로 위치하는 대각 행렬.
* $V^T$: 오른쪽 특이 벡터 (Orthogonal Matrix), 열 공간의 기저
* **특잇값(**$\sigma_i$**)의 역할:** $\Sigma$ 행렬의 대각 성분인 특잇값은 각 특이 벡터 축이 데이터에 담고 있는 **정보량 또는 분산의 크기**를 나타냅니다. 특잇값이 클수록 해당 축이 원본 행렬의 정보에서 더 중요한 부분을 설명합니다.
* **저랭크 근사 활용:** $\Sigma$ 행렬에서 **가장 큰** $k$**개의 특잇값만** 남기고 나머지를 0으로 처리하면, 원래 행렬 $M$을 랭크 $k$로 근사한 $\tilde{M}$을 얻을 수 있습니다. ($$\tilde{M} = U_k \Sigma_k V_k^T$$
  )

#### **누적 분산 (Cumulative Variance) 심화 설명**

\*\*누적 분산 설명 비율 (Cumulative PVE)\*\*은 SVD/PCA를 통해 몇 개의 주성분($k$)을 유지할 때 원본 데이터의 전체 분산(정보)을 얼마나 보존할 수 있는지를 정량적으로 평가하는 척도입니다. 이는 최적의 차원 수 $k$를 결정하는 과학적인 기준이 됩니다.

1. 분산 설명 비율 (Proportion of Variance Explained, PVE) 계산:
   각 특잇값($\sigma_i$)의 제곱($$\sigma_i^2$$
   )은 해당 차원이 설명하는 분산의 크기에 비례합니다. 전체 분산은 모든 특잇값의 제곱합으로 정의되며, $i$번째 특잇값의 PVE는 다음과 같습니다:
   $$PVE_i = \frac{\sigma_i^2}{\sum_{j=1}^{n} \sigma_j^2}$$
2. 누적 분산 설명 비율 (Cumulative PVE) 계산:
   $k$개의 주성분을 선택했을 때의 누적 PVE는 처음 $k$개 PVE의 합입니다.
   $$Cumulative PVE_k = \sum_{i=1}^{k} PVE_i$$
3. **최적** $k$ **선택 기준:**
   * **임계값 기준:** 누적 PVE가 90%, 95% 등 **미리 설정된 정보 보존 임계값**을 처음으로 넘어서는 지점의 $k$를 선택합니다. 이 방법은 정보 손실을 허용 가능한 범위 내로 최소화하려는 실무적 기준입니다.
   * **엘보우(Elbow) 기준:** PVE 값들을 그래프로 시각화했을 때, PVE가 급격히 감소하다가 그 기울기가 완만하게 둔화되는 지점(꺾이는 지점)을 \*\*엘보우 포인트(Elbow Point)\*\*라고 합니다. 이 지점 이후의 특잇값들은 데이터의 노이즈에 가깝거나 중요도가 낮다고 판단하여 해당 $k$를 최적의 랭크로 선택합니다. 이 방법은 시각화 기반의 주관적인 판단 기준을 제공합니다.

### **B. PCA (주성분 분석)**

데이터의 분산(Variance)을 가장 잘 설명하는 새로운 축(주성분, Principal Components)을 찾아 고차원 데이터를 저차원으로 투영하는 기법입니다.

* **PCA 수행 단계:**
  1. **데이터 표준화(Standardization):** 각 피처의 평균을 0, 분산을 1로 맞춥니다.
  2. **공분산 행렬 계산:** 피처들 간의 관계(공분산)를 파악합니다.
  3. **고유값 및 고유 벡터 계산:** 공분산 행렬을 분해하여 고유값이 큰 순서대로 고유 벡터(주성분)를 얻습니다. 고유 벡터가 새로운 축을 의미하고, 고유값은 해당 축이 담고 있는 분산의 크기를 나타냅니다.
  4. **데이터 투영:** 최종적으로 $k$개의 고유 벡터를 사용하여 원래 데이터를 새로운 저차원 공간에 투영하여 차원을 축소합니다.
* **SVD와의 관계:** PCA는 전통적으로 공분산 행렬의 \*\*고유값 분해(Eigendecomposition)\*\*를 통해 수행되지만, 데이터 행렬 자체에 **SVD를 적용**하여 주성분을 찾는 것이 훨씬 효율적이며 수치적으로 안정적입니다. 즉, SVD는 PCA를 계산하는 강력한 도구로 사용됩니다.
* **목적:** 불필요한 노이즈를 제거하고, 데이터의 핵심적인 구조를 파악하며, 모델 학습 속도를 높입니다.

## **3\. LoRA (Low-Rank Adaptation)와 저랭크 근사**

LoRA는 SVD를 통해 확립된 **저랭크 근사**의 원리를 거대 언어 모델(LLM) 튜닝에 적용하여 파인튜닝의 비용을 혁신적으로 줄인 기법입니다.

### **A. LoRA의 기본 원리**

기존의 LLM 가중치 행렬 $W_0$ 전체를 학습시키는 대신, $W_0$에 더해지는 \*\*업데이트 행렬 $\Delta W$\*\*만을 학습시킵니다. 이때 $\Delta W$는 다음과 같이 두 개의 작은 랭크 $r$을 가진 행렬 $A$와 $B$의 곱으로 분해됩니다.

$$\Delta W = B A$$

* $W$: 가중치 행렬 ($d \times k$ 차원)
* $B$: ($d \times r$ 차원), $A$: ($r \times k$ 차원)
* **랭크** $r$: $r$은 $d$와 $k$에 비해 매우 작은 값($r \ll \min(d, k)$)으로, $\Delta W$ 행렬의 실제 랭크를 결정하는 **LoRA의 핵심 하이퍼파라미터**입니다.

### **B. SVD/PCA와의 연결고리 및 MLOps 적용**

1. **아이디어의 출처:** LoRA는 **SVD가 행렬의 중요한 정보를 낮은 차원으로 압축할 수 있다**는 사실, 즉 모델이 특정 태스크를 학습하며 변화하는 업데이트 방향($\Delta W$) 역시 저랭크 행렬로 충분히 표현 가능하다는 가설을 기반으로 합니다.
2. **경량화의 효과:**
   * 기존 학습 파라미터 수: $d \times k$
   * LoRA 학습 파라미터 수:$$(d \times r) + (r \times k)$$
   * *예시:* 1000x1000 행렬($10^6$ 파라미터)을 랭크 $r=4$로 근사하면,$$(1000 \times 4) + (4 \times 1000) = 8000$$
     파라미터만 학습하면 됩니다. **파라미터 수를 99% 이상 절감**하여 학습 메모리 요구량(VRAM)과 시간을 크게 줄입니다.
3. **MLOps에서의 이점 (배포 및 관리):**
   * **배포 효율성:** Fine-Tuned 모델 전체를 다시 배포하는 대신, 몇 MB에 불과한 $A$와 $B$ 행렬($\Delta W$의 구성 요소)만 배포하면 됩니다.
   * **모델 버전 관리:** Task A, Task B, Task C별로 $A_i, B_i$ 쌍만 관리하면 되므로, 수십 개의 Fine-Tuned 모델 전체를 저장할 필요 없이 효율적인 \*\*모델 스위칭(Context Switching)\*\*이 가능해집니다.
   * **추론(Inference) 효율성:** 학습이 완료된 후 $W_{new} = W_0 + B A$를 계산하여 $W_{new}$를 저장하는 **Weight Merging**을 수행하면, 추론 시 추가적인 연산 오버헤드 없이 Fine-Tuned 모델처럼 사용할 수 있습니다.

## **4\. 예상 객관식/단답형 질문 (Self-Test)**

1. **질문:** 행렬의 랭크(Rank)를 기반으로 SVD를 수행할 때, **가장 큰** $k$**개의 특잇값**만을 보존하는 행렬 근사를 사용하는 주된 이유 두 가지는 무엇인가요?
   * **답변 예시:** 1\) **경량화/차원 축소:** 파라미터 수를 줄여 메모리 및 연산 효율을 높입니다. 2\) **노이즈 제거:** 작은 특잇값들은 주로 노이즈나 덜 중요한 정보를 나타내므로, 이를 제거하여 모델의 일반화 성능을 높일 수 있습니다.
2. **질문:** LoRA에서 **랭크** $r$**의 값**이 지나치게 커지거나(예: $r \approx \min(d, k)$) 지나치게 작아질 때($r=1$) 발생할 수 있는 문제점을 설명하세요.
   * **답변 예시:**
     * $r$**이 너무 클 때:** 학습해야 할 파라미터 수가 증가하여 LoRA의 핵심 이점인 **경량화 효과가 감소**합니다.
     * $r$**이 너무 작을 때:** $\Delta W$의 표현력이 떨어져 모델이 학습해야 할 **중요한 변화를 담아내지 못하여** 모델 성능이 낮아지는 **언더피팅**이 발생할 수 있습니다.
3. **질문:** PCA와 LoRA는 모두 저랭크 근사 원리를 사용하지만, **적용 대상과 목적**에서 어떤 차이가 있나요?
   * **답변 예시:**
     * **PCA:** 적용 대상은 \*\*데이터(특징)\*\*이며, 목적은 **데이터의 차원을 축소하고 시각화/노이즈를 제거**하는 것입니다.
     * **LoRA:** 적용 대상은 \*\*신경망 모델의 가중치 업데이트 행렬($\Delta W$)\*\*이며, 목적은 **파라미터 수를 줄여 모델의 튜닝/학습 효율을 극대화**하는 것입니다.
4. **질문:** LoRA 학습 후, 추론(Inference) 시 지연 시간(Latency) 증가 없이 파인튜닝된 효과를 얻기 위해 취할 수 있는 조치는 무엇이며, 이는 MLOps 관점에서 어떤 이점을 제공하나요?
   * **답변 예시:** **Weight Merging**을 수행합니다. 즉, 학습된 $\Delta W = B A$를 계산하여 기본 가중치 $W_0$에 더해 최종 가중치 $W_{new} = W_0 + \Delta W$를 만듭니다. 이 $W_{new}$를 추론에 사용하면, LoRA 모듈을 거치는 추가 연산 없이 일반 파인튜닝 모델과 동일한 속도로 추론이 가능합니다. 이점은 추론 서버의 복잡성을 줄이고, 여러 태스크에 대한 모델을 배포할 때 메모리 효율성을 극대화한다는 것입니다.
